experiment:
  name: env(LLAMA_MODEL:llama-7b)-wdsimple

seed: 22

input_tokenizer: file(tokenizers/env(LLAMA_TOKENIZER:llama).yaml)
output_tokenizer: file(tokenizers/env(LLAMA_TOKENIZER:llama).yaml)

model: file(models/llama.yaml)

train:
  peft: file(peft/env(PEFT:none).yaml)
  compile: env(COMPILE:false)
  precision: env(PRECISION:fp32)
  clip_grad_norm: env(CLIP_GRAD_NORM:1.0)
  num_epochs: env(NUM_EPOCHS:1)
  eval_interval: eval(1 / env(EVAL_N:4))
  log_interval: eval(1 / env(LOG_N:100))
  step_interval: eval(1 / env(STEP_N:100000))
  mask_prefix: env(MASK_PREFIX:true)
  distributed:
    type: env(DIST_TYPE:DDP)
    strategy: env(DIST_SHARD:NO_SHARD)
    offload: env(DIST_OFFLOAD:false)
    prefetch: env(DIST_PREFETCH:true)
    shard_size: env(DIST_SHARD_SIZE:null)
  loss:
    type: sequence
    loss:
      type: cross_entropy
      ignore_index: -1
      label_smoothing: env(LABEL_SMOOTHING:0.0)
  optimizer:
    type: adamw
    lr: env(LR:0.00003)
    weight_decay: env(WEIGHT_DECAY:0.01)
  lr_scheduler:
    # type: cont_sqrt_with_warmup
    # warmup_steps: eval((40454 * env(NUM_EPOCHS:1)) // (100 * env(BATCH_LIMIT:8)))
    type: constant_with_warmup
    warmup_steps: env(WARMUP:0.01)
    # type: multi_step_with_warmup
    # warmup_steps: env(WARMUP_STEPS:0.01)
    # steps: [0.9]
    # factors: [0.1]
  data:
    strategy: weighted
    shuffle: true
    sort: env(SORT:false)
    limit: env(TRAIN_LIMIT:null)
    max_length: env(MAX_LENGTH:2048)
    buffer_size: env(BUFFER_SIZE:512)
    prefetch_factor: env(PREFETCH_FACTOR:512)
    num_threads: eval(env(THREADS:None) or len(os.sched_getaffinity(0)) // 2)
    batch_limit: eval(env(MAX_LENGTH:512) * env(BATCH_LIMIT:8) if "env(BATCH_LIMIT_TYPE:batch_size)" == "padded_item_size" else env(BATCH_LIMIT:8))
    batch_limit_type: env(BATCH_LIMIT_TYPE:batch_size)
    pipeline:
      preprocessing: file(preprocessings/clean_and_normalize.yaml)
      tokenizer: file(tokenizers/env(LLAMA_TOKENIZER:llama)_train.yaml)
      labeling:
        tokenizer: file(tokenizers/env(LLAMA_TOKENIZER:llama)_train.yaml)
        type: generation
        generation:
          type: input_and_target
          separator: ": "
      postprocessing: file(postprocessings/clip.yaml)
    sources: 
      - file(data/wikidata_simplequestions.yaml)
val:
  cooldown: env(COOLDOWN:0.05)
  data: 
    - file(data/env(DATA_SOURCES:wikidata_simplequestions_val).yaml)
  benchmark:
    limit: env(BENCHMARK_LIMIT:128)
    entity_index: data/prefix-index/wikidata-llama-entities.bin
    property_index: data/prefix-index/wikidata-llama-properties.bin
    search: env(SEARCH:beam)
    beam_width: env(BEAM_WIDTH:4)
    batch_size: eval(env(BATCH_LIMIT:8) // env(BEAM_WIDTH:4))
    kg: wikidata
    log_n_samples: env(BENCHMARK_LOG_N:8)
